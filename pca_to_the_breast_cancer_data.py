# -*- coding: utf-8 -*-
"""PCA to the Breast Cancer Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hd1iD22mkNZ8Nw16siJ7MAN37QyPWtCH
"""

#PCA to the Breast Cancer Data

# Importamos las bibliotecas
from google.colab import files
import pandas as pd


dataset = pd.read_csv('data.csv')


# Mostramos las primeras filas del DataFrame para verificar que todo esté bien
dataset.head()

# Importamos las librerías necesarias
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Cargar el archivo CSV
df = pd.read_csv('data.csv')  # Actualiza la ruta del archivo si es necesario

# Eliminar la columna 'Unnamed: 32' que no contiene datos útiles
df = df.drop(columns=['Unnamed: 32'])

# Convertir la columna 'diagnosis' a valores numéricos: 0 para benigno (B) y 1 para maligno (M)
df['diagnosis'] = df['diagnosis'].map({'B': 0, 'M': 1})

# Seleccionar las características (columnas) para el análisis
X = df.iloc[:, 2:].values  # Excluimos las primeras dos columnas ('id' y 'diagnosis')
y = df['diagnosis'].values

# Seleccionamos las 10 primeras características y la columna de diagnóstico
features = data_new[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
                     'smoothness_mean', 'compactness_mean', 'concavity_mean',
                     'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]
labels = data_new['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)  # Convertir M=1 y B=0

# Escalamos las características
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Dividir el conjunto de datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresión logística con las 10 características
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(X_train, y_train)

# Predecir sobre el conjunto de prueba
y_pred = log_reg.predict(X_test)

# Calcular la precisión
accuracy_10_features = accuracy_score(y_test, y_pred)
accuracy_10_features

# Aplicar PCA para reducir las dimensiones a 1 (PC1)
pca = PCA(n_components=1)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Entrenar el modelo de regresión logística con solo PC1
log_reg.fit(X_train_pca, y_train)

# Predecir sobre el conjunto de prueba usando PC1
y_pred_pca = log_reg.predict(X_test_pca)

# Calcular la precisión con PC1
accuracy_pc1 = accuracy_score(y_test, y_pred_pca)
accuracy_pc1

# Entrenar un modelo de regresión logística para cada característica individualmente
accuracies = []

for i in range(features_scaled.shape[1]):
    # Usamos solo una característica
    X_single_feature = features_scaled[:, i].reshape(-1, 1)

    # Dividimos los datos en entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X_single_feature, labels, test_size=0.3, random_state=42)

    # Inicializamos el modelo
    model = LogisticRegression()

    # Entrenamos el modelo
    model.fit(X_train, y_train)

    # Hacemos predicciones
    y_pred = model.predict(X_test)

    # Calculamos el accuracy
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Crear un DataFrame para visualizar los resultados
results = pd.DataFrame({
    'Feature': features.columns,
    'Accuracy': accuracies
})

results

# Crear un gráfico de barras para visualizar las accuracies por cada característica
plt.figure(figsize=(10, 6))
plt.bar(results['Feature'], results['Accuracy'], color='skyblue')
plt.xlabel('Características')
plt.ylabel('Accuracy')
plt.title('Accuracy de Regresión Logística por Característica')
plt.xticks(rotation=45, ha='right')
plt.show()